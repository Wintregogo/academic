# app.py
import streamlit as st
import yaml
import os
from datetime import datetime
from utils import load_config  # â† æ–°å¢å¯¼å…¥
from main_streamlit import run_analysis, streaming_run_analysis

# === 1. åŠ è½½é»˜è®¤é…ç½® ===
DEFAULT_CONFIG = load_config("config.yaml")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="arXiv Insight",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” arXiv é¢„å°æœ¬æ™ºèƒ½åˆ†æç³»ç»Ÿ")
st.markdown("è¾“å…¥å…³é”®è¯ï¼Œè‡ªåŠ¨è·å–æœ€æ–°è®ºæ–‡å¹¶ç”± LLM è¯„åˆ†ã€æå–äº®ç‚¹")

# === 2. ä¾§è¾¹æ ï¼šä½¿ç”¨ DEFAULT_CONFIG å¡«å……é»˜è®¤å€¼ ===
with st.sidebar:
    st.header("âš™ï¸ é…ç½®å‚æ•°")

    # æŸ¥è¯¢å‚æ•°
    default_keywords = ", ".join(DEFAULT_CONFIG.get("query", {}).get("keywords", ["large language models"]))
    keywords = st.text_input("å…³é”®è¯ï¼ˆè‹±æ–‡ï¼Œé€—å·åˆ†éš”ï¼‰", value=default_keywords)

    default_cats = DEFAULT_CONFIG.get("query", {}).get("categories", ["cs.CL", "cs.AI"])
    categories = st.multiselect(
        "å­¦ç§‘åˆ†ç±»",
        options=["cs.CL", "cs.AI", "cs.LG", "cs.CV", "stat.ML", "physics.comp-ph"],
        default=default_cats
    )

    default_days = DEFAULT_CONFIG.get("query", {}).get("time_window_days", 7)
    days = st.slider("æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰", 1, 30, default_days)

    default_topk = DEFAULT_CONFIG.get("query", {}).get("top_k", 5)
    top_k = st.slider("è¿”å›ç¯‡æ•°", 1, 20, default_topk)

    # ä½œè€…ä¿¡æ¯
    author_cfg = DEFAULT_CONFIG.get("features", {}).get("author_info", {})
    use_author_info = st.checkbox(
        "å¯ç”¨ä½œè€…ä¿¡æ¯ï¼ˆSemantic Scholar / OpenAlexï¼‰",
        value=author_cfg.get("enabled", False)
    )
    
    default_sources = author_cfg.get("sources", ["semantic_scholar", "openalex"])
    author_sources = []
    if use_author_info:
        author_sources = st.multiselect(
            "ä½œè€…æ•°æ®æºï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰",
            options=["semantic_scholar", "openalex"],
            default=[s for s in default_sources if s in ["semantic_scholar", "openalex"]]
        )

    # è§£æå™¨
    parser_cfg = DEFAULT_CONFIG.get("parser", {})
    use_grobid = st.checkbox("ä½¿ç”¨ Grobid è§£æ PDF", value=parser_cfg.get("use_grobid", False))

    # LLM è®¾ç½®
    llm_cfg = DEFAULT_CONFIG.get("llm", {})
    llm_provider = st.selectbox("æ¨¡å‹æä¾›å•†", ["qwen"], index=0)  # ç›®å‰åªæ”¯æŒ qwen
    default_model = llm_cfg.get("model", "qwen-plus")
    llm_model = st.selectbox(
        "æ¨¡å‹",
        ["qwen-turbo", "qwen-plus", "qwen-max"],
        index=["qwen-turbo", "qwen-plus", "qwen-max"].index(default_model) if default_model in ["qwen-turbo", "qwen-plus", "qwen-max"] else 1
    )
    api_key = st.text_input("DashScope API Key", type="password", value=llm_cfg.get("api_key", ""))

    output_cfg = DEFAULT_CONFIG.get("output", {})
    report_path = output_cfg.get("report_path", "export/report.md")
    csv_path = output_cfg.get("csv_path", "export/report.csv")
    json_path = output_cfg.get("json_path", "export/report.json")

    run_btn = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary")

# === 3. ä¸»é€»è¾‘ï¼šæ„å»ºæœ€ç»ˆ configï¼ˆUI è¦†ç›–é»˜è®¤ï¼‰===
if run_btn:
    if not api_key:
        st.error("è¯·å¡«å†™ DashScope API Key")
    else:
        # æ„å»ºæœ€ç»ˆé…ç½®ï¼šä»¥ DEFAULT_CONFIG ä¸ºåŸºç¡€ï¼Œç”¨ UI å€¼è¦†ç›–
        config = {
            "query": {
                "keywords": [k.strip() for k in keywords.split(",") if k.strip()],
                "categories": categories,
                "time_window_days": days,
                "top_k": top_k
            },
            "llm": {
                "provider": llm_provider,
                "model": llm_model,
                "api_key": api_key
            },
            "parser": {
                "use_grobid": use_grobid,
                "grobid_url": "http://localhost:8070"
            },
            "output": {
                "report_path": report_path,
                "csv_path": csv_path,
                "json_path": json_path
            },
            "features": {
                "author_info": {
                    "enabled": use_author_info,
                    "sources": author_sources if use_author_info else []
                }
            }
        }

        # === åˆ›å»ºåŠ¨æ€æ›´æ–°åŒºåŸŸ ===
        status_container = st.empty()
        results_container = st.empty()
        download_container = st.empty()

        all_results = []

        with st.spinner("æ­£åœ¨åˆ†æè®ºæ–‡..."):
            try:
                # æµå¼å¤„ç†
                for partial_results in streaming_run_analysis(config):
                    all_results = partial_results  # ä¿ç•™æœ€æ–°çŠ¶æ€

                    # æ›´æ–°çŠ¶æ€
                    status_container.info(f"â³ å·²åˆ†æ {len(partial_results)} ç¯‡è®ºæ–‡ï¼Œæ­£åœ¨æ’åº...")

                    # æ¸…ç©ºå¹¶é‡ç»˜ç»“æœï¼ˆåªæ˜¾ç¤ºå½“å‰ top_kï¼‰
                    top_k = config["query"]["top_k"]
                    display_papers = sorted(partial_results, key=lambda x: x.get("final_score", 0), reverse=True)[:top_k]

                    results_container.empty()  # æ¸…ç©ºæ—§å†…å®¹
                    with results_container.container():
                        for i, paper in enumerate(display_papers):
                            with st.expander(f"{i+1}. {paper['title']}", expanded=(i == 0)):
                                col1, col2 = st.columns([2, 1])
                                with col1:
                                    st.markdown(f"**å‘è¡¨æ—¶é—´**: {paper['published'][:10]}")
                                    st.markdown(f"**åˆ†æ•°**: `{paper.get('final_score', 0)}` (åŸºç¡€: `{paper.get('total_score', 0)}`, äº®ç‚¹: `{paper.get('insight_bonus', 0)}`)")
                                    st.markdown(f"**è¯­è¨€**: {'ä¸­æ–‡' if paper.get('language') == 'zh' else 'English'}")
                                    st.markdown(f"[æŸ¥çœ‹å…¨æ–‡](https://arxiv.org/abs/{paper['id']}) | [PDF](https://arxiv.org/pdf/{paper['id']})")
                                    
                                    if paper.get("authors_info"):
                                        st.markdown("**ä½œè€…ä¿¡æ¯**:")
                                        for author in paper["authors_info"]:
                                            name = author.get("name", "N/A")
                                            hindex = author.get("h_index", "N/A")
                                            org = author.get("affiliations", ["N/A"])[0] if author.get("affiliations") else "N/A"
                                            source = author.get("source_used", "")
                                            st.caption(f"- {name} | H-index: {hindex} | {org} ({source})")

                                    st.markdown("**æ‘˜è¦**:")
                                    st.write(paper["abstract"])
                                    st.markdown("**ğŸ’¡ äº®ç‚¹**:")
                                    st.write(paper["breakthrough"])

                                with col2:
                                    st.metric("åˆ›æ–°æ€§", paper.get("innovation", 0))
                                    st.metric("ä¸¥è°¨æ€§", paper.get("rigor", 0))
                                    st.metric("å½±å“åŠ›", paper.get("impact", 0))

                # === å…¨éƒ¨å®Œæˆå ===
                status_container.success(f"âœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {len(all_results)} ç¯‡è®ºæ–‡")

                # æä¾›ä¸‹è½½
                import pandas as pd
                df = pd.DataFrame(all_results)
                csv = df.to_csv(index=False).encode('utf-8-sig')
                download_container.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´ CSV", csv, "arxiv_insight.csv", "text/csv")

            except Exception as e:
                status_container.error(f"åˆ†æå‡ºé”™: {str(e)}")
                st.exception(e)